{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2efc9943",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, we initialize JPype to access NeqSim's Java classes directly from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b74229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install neqsim numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f9a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NeqSim - Direct Java Access via jneqsim\n",
    "from neqsim import jneqsim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"NeqSim loaded via jneqsim gateway\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf9c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NeqSim ML classes through jneqsim gateway\n",
    "StateVector = jneqsim.process.ml.StateVector\n",
    "ActionVector = jneqsim.process.ml.ActionVector\n",
    "Constraint = jneqsim.process.ml.Constraint\n",
    "ConstraintManager = jneqsim.process.ml.ConstraintManager\n",
    "GymEnvironment = jneqsim.process.ml.GymEnvironment\n",
    "RLEnvironment = jneqsim.process.ml.RLEnvironment\n",
    "TrainingDataCollector = jneqsim.process.ml.TrainingDataCollector\n",
    "\n",
    "# ML examples\n",
    "SeparatorGymEnv = jneqsim.process.ml.examples.SeparatorGymEnv\n",
    "SeparatorLevelControlEnv = jneqsim.process.ml.examples.SeparatorLevelControlEnv\n",
    "SeparatorCompressorMultiAgentEnv = jneqsim.process.ml.examples.SeparatorCompressorMultiAgentEnv\n",
    "MultiAgentEnvironment = jneqsim.process.ml.multiagent.MultiAgentEnvironment\n",
    "\n",
    "# Import process equipment\n",
    "Separator = jneqsim.process.equipment.separator.Separator\n",
    "Compressor = jneqsim.process.equipment.compressor.Compressor\n",
    "Stream = jneqsim.process.equipment.stream.Stream\n",
    "SystemSrkEos = jneqsim.thermo.system.SystemSrkEos\n",
    "\n",
    "print(\"NeqSim ML classes imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b5a3da",
   "metadata": {},
   "source": [
    "## 2. StateVector - Normalized State Representation\n",
    "\n",
    "The `StateVector` class provides:\n",
    "- Automatic normalization to [0, 1] for neural network inputs\n",
    "- Physical bounds tracking\n",
    "- Unit awareness\n",
    "- JSON/array export for ML frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f11765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a state vector manually\n",
    "state = StateVector()\n",
    "state.add(\"temperature\", 350.0, 200.0, 500.0, \"K\")  # value, min, max, unit\n",
    "state.add(\"pressure\", 50.0, 0.0, 100.0, \"bar\")\n",
    "state.add(\"liquid_level\", 0.6, 0.0, 1.0, \"fraction\")\n",
    "\n",
    "# Access values\n",
    "print(f\"Temperature: {state.getValue('temperature'):.1f} K\")\n",
    "print(f\"Temperature (normalized): {state.getNormalized('temperature'):.3f}\")\n",
    "\n",
    "# Convert to numpy array for ML\n",
    "raw_array = np.array(state.toArray())\n",
    "norm_array = np.array(state.toNormalizedArray())\n",
    "\n",
    "print(f\"\\nRaw array: {raw_array}\")\n",
    "print(f\"Normalized array: {norm_array}\")\n",
    "print(f\"Feature names: {list(state.getFeatureNames())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034a81d",
   "metadata": {},
   "source": [
    "## 3. Equipment StateVectorProvider\n",
    "\n",
    "Key equipment classes (Separator, Compressor, HeatExchanger) implement `StateVectorProvider` interface, enabling direct state extraction for RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b439f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple separation process\n",
    "fluid = SystemSrkEos(280.0, 50.0)  # T=280K, P=50bar\n",
    "fluid.addComponent(\"methane\", 0.8)\n",
    "fluid.addComponent(\"ethane\", 0.1)\n",
    "fluid.addComponent(\"propane\", 0.05)\n",
    "fluid.addComponent(\"n-pentane\", 0.05)\n",
    "fluid.setMixingRule(\"classic\")\n",
    "fluid.setMultiPhaseCheck(True)\n",
    "\n",
    "# Create and run separator\n",
    "feed = Stream(\"Feed\", fluid)\n",
    "feed.setFlowRate(1000.0, \"kg/hr\")\n",
    "feed.run()\n",
    "\n",
    "separator = Separator(\"Sep-001\", feed)\n",
    "separator.run()\n",
    "\n",
    "# Extract state vector directly from equipment\n",
    "sep_state = separator.getStateVector()\n",
    "\n",
    "print(\"Separator State Vector:\")\n",
    "print(\"-\" * 40)\n",
    "for name in sep_state.getFeatureNames():\n",
    "    value = sep_state.getValue(name)\n",
    "    norm = sep_state.getNormalized(name)\n",
    "    print(f\"{name:20s}: {value:10.3f} (norm: {norm:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e4a09",
   "metadata": {},
   "source": [
    "## 4. Constraint Management\n",
    "\n",
    "The `ConstraintManager` handles physical and safety constraints with:\n",
    "- **HARD** constraints: Terminate episode on violation (safety limits)\n",
    "- **SOFT** constraints: Penalize reward (operational targets)\n",
    "- Automatic penalty computation for RL rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd88691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup constraints\n",
    "constraints = ConstraintManager()\n",
    "\n",
    "# Hard constraints (safety limits)\n",
    "constraints.addHardRange(\"max_pressure\", \"pressure\", 0.0, 80.0, \"bar\")\n",
    "constraints.addHardRange(\"level_bounds\", \"liquid_level\", 0.1, 0.9, \"fraction\")\n",
    "\n",
    "# Soft constraints (operational targets)\n",
    "constraints.addSoftRange(\"optimal_level\", \"liquid_level\", 0.4, 0.6, \"fraction\")\n",
    "\n",
    "# Test against different states\n",
    "test_cases = [\n",
    "    {\"pressure\": 50.0, \"liquid_level\": 0.5},  # All OK\n",
    "    {\"pressure\": 50.0, \"liquid_level\": 0.75}, # Soft violation\n",
    "    {\"pressure\": 90.0, \"liquid_level\": 0.5},  # Hard violation!\n",
    "]\n",
    "\n",
    "for i, case in enumerate(test_cases):\n",
    "    state = StateVector()\n",
    "    state.add(\"pressure\", case[\"pressure\"], 0.0, 100.0, \"bar\")\n",
    "    state.add(\"liquid_level\", case[\"liquid_level\"], 0.0, 1.0, \"fraction\")\n",
    "    \n",
    "    constraints.evaluate(state)\n",
    "    \n",
    "    print(f\"\\nCase {i+1}: P={case['pressure']} bar, Level={case['liquid_level']}\")\n",
    "    print(f\"  Hard violation: {constraints.hasHardViolation()}\")\n",
    "    print(f\"  Penalty: {constraints.getTotalViolationPenalty():.2f}\")\n",
    "    if constraints.getViolations().size() > 0:\n",
    "        print(f\"  Explanation: {constraints.explainViolations()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77774b4d",
   "metadata": {},
   "source": [
    "## 5. Single-Agent Gym Environment\n",
    "\n",
    "The `SeparatorGymEnv` provides a Gymnasium-compatible interface:\n",
    "\n",
    "**Observation Space (8-dim):**\n",
    "- liquid_level, pressure, temperature, feed_flow\n",
    "- gas_density, liquid_density, level_error, valve_position\n",
    "\n",
    "**Action Space (1-dim):**\n",
    "- valve_delta: Change in valve position [-0.1, 0.1]\n",
    "\n",
    "**Reward:**\n",
    "- Setpoint tracking: $-10 \\times (level - setpoint)^2$\n",
    "- Action smoothness: $-0.1 \\times action^2$\n",
    "- Survival bonus: $+1.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc3288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gym environment\n",
    "env = SeparatorGymEnv()\n",
    "env.setMaxEpisodeSteps(200)\n",
    "env.setLevelSetpoint(0.5)\n",
    "\n",
    "print(\"Environment Specification:\")\n",
    "print(f\"  Observation dim: {env.getObservationDim()}\")\n",
    "print(f\"  Action dim: {env.getActionDim()}\")\n",
    "print(f\"  Observation names: {list(env.getObservationNames())}\")\n",
    "print(f\"  Action bounds: [{env.getActionLow()[0]}, {env.getActionHigh()[0]}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5718b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run episode with simple P-controller\n",
    "def simple_controller(obs, setpoint=0.5, Kp=0.5):\n",
    "    \"\"\"Simple proportional controller for level control.\"\"\"\n",
    "    level = obs[0]  # liquid_level is first observation\n",
    "    error = setpoint - level\n",
    "    action = Kp * error\n",
    "    return np.clip(action, -0.1, 0.1)\n",
    "\n",
    "# Run episode\n",
    "reset_result = env.reset()\n",
    "obs = np.array(reset_result.observation)\n",
    "\n",
    "history = {\n",
    "    'level': [], 'valve': [], 'reward': [], 'action': []\n",
    "}\n",
    "\n",
    "total_reward = 0\n",
    "step = 0\n",
    "\n",
    "while not env.isDone():\n",
    "    # Get action from controller\n",
    "    action = simple_controller(obs)\n",
    "    \n",
    "    # Step environment\n",
    "    result = env.step([float(action)])\n",
    "    obs = np.array(result.observation)\n",
    "    \n",
    "    # Record history\n",
    "    history['level'].append(obs[0])\n",
    "    history['valve'].append(obs[7])\n",
    "    history['reward'].append(result.reward)\n",
    "    history['action'].append(action)\n",
    "    total_reward += result.reward\n",
    "    step += 1\n",
    "\n",
    "print(f\"Episode finished after {step} steps\")\n",
    "print(f\"Total reward: {total_reward:.2f}\")\n",
    "print(f\"Final level: {history['level'][-1]:.3f} (setpoint: 0.5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab7742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "axes[0, 0].plot(history['level'], 'b-', linewidth=2)\n",
    "axes[0, 0].axhline(y=0.5, color='r', linestyle='--', label='Setpoint')\n",
    "axes[0, 0].set_ylabel('Liquid Level')\n",
    "axes[0, 0].set_xlabel('Step')\n",
    "axes[0, 0].set_title('Level Control')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(history['valve'], 'g-', linewidth=2)\n",
    "axes[0, 1].set_ylabel('Valve Position')\n",
    "axes[0, 1].set_xlabel('Step')\n",
    "axes[0, 1].set_title('Valve Position')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(history['action'], 'm-', linewidth=1)\n",
    "axes[1, 0].set_ylabel('Action (valve delta)')\n",
    "axes[1, 0].set_xlabel('Step')\n",
    "axes[1, 0].set_title('Control Actions')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(np.cumsum(history['reward']), 'orange', linewidth=2)\n",
    "axes[1, 1].set_ylabel('Cumulative Reward')\n",
    "axes[1, 1].set_xlabel('Step')\n",
    "axes[1, 1].set_title('Reward Accumulation')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37fdd71",
   "metadata": {},
   "source": [
    "## 6. Multi-Agent Environment\n",
    "\n",
    "The `MultiAgentEnvironment` coordinates multiple agents controlling interconnected equipment.\n",
    "\n",
    "### Example: Separator + Compressor Train\n",
    "\n",
    "```\n",
    "Feed ──► [Separator] ──► Gas ──► [Compressor] ──► Compressed Gas\n",
    "              │\n",
    "              └──► Liquid ──► [Valve] ──► Export\n",
    "```\n",
    "\n",
    "**Agents:**\n",
    "1. **SeparatorAgent**: Controls liquid level via outlet valve\n",
    "2. **CompressorAgent**: Controls discharge pressure via speed\n",
    "\n",
    "**Coordination Modes:**\n",
    "- `INDEPENDENT`: Each agent optimizes local reward\n",
    "- `COOPERATIVE`: All agents share team reward\n",
    "- `CTDE`: Centralized training, decentralized execution\n",
    "- `COMMUNICATING`: Agents exchange messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bcf514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-agent environment\n",
    "multi_env = SeparatorCompressorMultiAgentEnv()\n",
    "multi_env.setCoordinationMode(MultiAgentEnvironment.CoordinationMode.COOPERATIVE)\n",
    "multi_env.setMaxEpisodeSteps(100)\n",
    "\n",
    "print(\"Multi-Agent Environment:\")\n",
    "print(f\"  Number of agents: {multi_env.getNumAgents()}\")\n",
    "print(f\"  Agent IDs: {list(multi_env.getAgentIds())}\")\n",
    "\n",
    "# Get agent details\n",
    "for agent_id in multi_env.getAgentIds():\n",
    "    agent = multi_env.getAgent(agent_id)\n",
    "    print(f\"\\n  {agent_id}:\")\n",
    "    print(f\"    Observation dim: {agent.getObservationDim()}\")\n",
    "    print(f\"    Action dim: {agent.getActionDim()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de322c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multi-agent episode\n",
    "obs = multi_env.reset()\n",
    "\n",
    "multi_history = {\n",
    "    'sep_reward': [], 'comp_reward': [],\n",
    "    'team_reward': []\n",
    "}\n",
    "\n",
    "step = 0\n",
    "while not multi_env.isDone() and step < 100:\n",
    "    # Simple policies for each agent\n",
    "    sep_obs = np.array(obs.get(\"separator\"))\n",
    "    comp_obs = np.array(obs.get(\"compressor\"))\n",
    "    \n",
    "    # Separator: P-control on level error (obs[2])\n",
    "    sep_action = -0.3 * sep_obs[2] if len(sep_obs) > 2 else 0.0\n",
    "    sep_action = np.clip(sep_action, -0.1, 0.1)\n",
    "    \n",
    "    # Compressor: P-control on pressure error (obs[6])\n",
    "    comp_action = -0.2 * comp_obs[6] if len(comp_obs) > 6 else 0.0\n",
    "    comp_action = np.clip(comp_action, -0.05, 0.05)\n",
    "    \n",
    "    # Create action dict\n",
    "    actions = {\n",
    "        \"separator\": [float(sep_action)],\n",
    "        \"compressor\": [float(comp_action)]\n",
    "    }\n",
    "    \n",
    "    # Step\n",
    "    result = multi_env.step(actions)\n",
    "    obs = result.observations\n",
    "    \n",
    "    # Record\n",
    "    multi_history['sep_reward'].append(result.rewards.get(\"separator\"))\n",
    "    multi_history['comp_reward'].append(result.rewards.get(\"compressor\"))\n",
    "    multi_history['team_reward'].append(\n",
    "        result.rewards.get(\"separator\") + result.rewards.get(\"compressor\")\n",
    "    )\n",
    "    step += 1\n",
    "\n",
    "print(f\"Multi-agent episode finished after {step} steps\")\n",
    "print(f\"Total team reward: {sum(multi_history['team_reward']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85478a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot multi-agent results\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(np.cumsum(multi_history['sep_reward']), 'b-', \n",
    "        label='Separator Agent', linewidth=2)\n",
    "ax.plot(np.cumsum(multi_history['comp_reward']), 'r-', \n",
    "        label='Compressor Agent', linewidth=2)\n",
    "ax.plot(np.cumsum(multi_history['team_reward']), 'g--', \n",
    "        label='Team Total', linewidth=2, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Cumulative Reward')\n",
    "ax.set_title('Multi-Agent Cooperative Rewards')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fda217",
   "metadata": {},
   "source": [
    "## 7. Training Data Collection for Surrogates\n",
    "\n",
    "The `TrainingDataCollector` generates datasets for training neural network surrogates of expensive thermodynamic calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1625067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neqsim.thermodynamicoperations import ThermodynamicOperations\n",
    "\n",
    "# Create collector for flash calculation surrogate\n",
    "collector = TrainingDataCollector(\"flash_surrogate\")\n",
    "collector.defineInput(\"temperature\", \"K\", 250.0, 350.0)\n",
    "collector.defineInput(\"pressure\", \"bar\", 10.0, 80.0)\n",
    "collector.defineOutput(\"vapor_fraction\", \"mole_frac\", 0.0, 1.0)\n",
    "collector.defineOutput(\"gas_density\", \"kg/m3\", 0.0, 100.0)\n",
    "\n",
    "# Generate training data\n",
    "fluid = SystemSrkEos(280.0, 50.0)\n",
    "fluid.addComponent(\"methane\", 0.85)\n",
    "fluid.addComponent(\"ethane\", 0.10)\n",
    "fluid.addComponent(\"propane\", 0.05)\n",
    "fluid.setMixingRule(\"classic\")\n",
    "\n",
    "ops = ThermodynamicOperations(fluid)\n",
    "\n",
    "# Sample grid\n",
    "n_samples = 0\n",
    "for T in np.linspace(250, 350, 15):\n",
    "    for P in np.linspace(10, 80, 15):\n",
    "        try:\n",
    "            fluid.setTemperature(T, \"K\")\n",
    "            fluid.setPressure(P, \"bar\")\n",
    "            ops.TPflash()\n",
    "            \n",
    "            collector.startSample()\n",
    "            collector.recordInput(\"temperature\", T)\n",
    "            collector.recordInput(\"pressure\", P)\n",
    "            \n",
    "            if fluid.getNumberOfPhases() > 0:\n",
    "                vapor_frac = fluid.getPhase(0).getBeta()\n",
    "                gas_density = fluid.getPhase(0).getDensity(\"kg/m3\")\n",
    "            else:\n",
    "                vapor_frac = 0.0\n",
    "                gas_density = 0.0\n",
    "            \n",
    "            collector.recordOutput(\"vapor_fraction\", vapor_frac)\n",
    "            collector.recordOutput(\"gas_density\", gas_density)\n",
    "            collector.endSample()\n",
    "            n_samples += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(f\"Collected {n_samples} samples\")\n",
    "print(f\"\\nInput statistics:\")\n",
    "for name, stats in collector.getInputStatistics().items():\n",
    "    print(f\"  {name}: mean={stats['mean']:.2f}, std={stats['std']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583515a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV (can be used with PyTorch/TensorFlow)\n",
    "csv_data = collector.toCSV()\n",
    "print(\"CSV Preview (first 500 chars):\")\n",
    "print(csv_data[:500])\n",
    "\n",
    "# Save to file\n",
    "# collector.exportCSV(\"flash_training_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b85c74",
   "metadata": {},
   "source": [
    "## 8. Integration with stable-baselines3\n",
    "\n",
    "Here's a template for training with popular RL libraries.\n",
    "\n",
    "### Gym Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d90ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class NeqSimGymWrapper(gym.Env):\n",
    "    \"\"\"Gymnasium wrapper for NeqSim environments.\"\"\"\n",
    "    \n",
    "    def __init__(self, java_env):\n",
    "        super().__init__()\n",
    "        self.java_env = java_env\n",
    "        \n",
    "        # Define spaces\n",
    "        obs_dim = java_env.getObservationDim()\n",
    "        act_dim = java_env.getActionDim()\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array(java_env.getObservationLow()),\n",
    "            high=np.array(java_env.getObservationHigh()),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array(java_env.getActionLow()),\n",
    "            high=np.array(java_env.getActionHigh()),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        result = self.java_env.reset()\n",
    "        obs = np.array(result.observation, dtype=np.float32)\n",
    "        info = dict(result.info)\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        result = self.java_env.step(list(action.astype(float)))\n",
    "        obs = np.array(result.observation, dtype=np.float32)\n",
    "        reward = float(result.reward)\n",
    "        terminated = bool(result.terminated)\n",
    "        truncated = bool(result.truncated)\n",
    "        info = dict(result.info)\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "# Create wrapped environment\n",
    "java_env = SeparatorGymEnv()\n",
    "java_env.setMaxEpisodeSteps(200)\n",
    "wrapped_env = NeqSimGymWrapper(java_env)\n",
    "\n",
    "print(f\"Observation space: {wrapped_env.observation_space}\")\n",
    "print(f\"Action space: {wrapped_env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231bc875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Training with stable-baselines3 (uncomment to run)\n",
    "# from stable_baselines3 import PPO\n",
    "# from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# # Validate environment\n",
    "# check_env(wrapped_env)\n",
    "\n",
    "# # Train PPO agent\n",
    "# model = PPO(\"MlpPolicy\", wrapped_env, verbose=1)\n",
    "# model.learn(total_timesteps=10000)\n",
    "\n",
    "# # Evaluate\n",
    "# obs, _ = wrapped_env.reset()\n",
    "# for _ in range(200):\n",
    "#     action, _ = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, terminated, truncated, info = wrapped_env.step(action)\n",
    "#     if terminated or truncated:\n",
    "#         break\n",
    "\n",
    "print(\"stable-baselines3 integration ready!\")\n",
    "print(\"Uncomment the code above to train a PPO agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849441b6",
   "metadata": {},
   "source": [
    "## 9. Java-Only Testing (No Python ML Required)\n",
    "\n",
    "NeqSim includes simple controllers that can be used to test the RL infrastructure directly from Java without requiring Python or ML libraries.\n",
    "\n",
    "### Available Controllers:\n",
    "\n",
    "| Controller | Description | Use Case |\n",
    "|------------|-------------|----------|\n",
    "| `ProportionalController` | P-control: action = -Kp × error | Simple setpoint tracking |\n",
    "| `PIDController` | PID with anti-windup | Industrial control baseline |\n",
    "| `BangBangController` | On-off with hysteresis | Simple threshold control |\n",
    "| `RandomController` | Uniform random actions | Baseline comparison |\n",
    "\n",
    "### Java Usage Example:\n",
    "\n",
    "```java\n",
    "import neqsim.process.ml.EpisodeRunner;\n",
    "import neqsim.process.ml.controllers.*;\n",
    "import neqsim.process.ml.examples.SeparatorGymEnv;\n",
    "\n",
    "// Create environment\n",
    "SeparatorGymEnv env = new SeparatorGymEnv();\n",
    "env.setMaxEpisodeSteps(500);\n",
    "\n",
    "// Create controllers\n",
    "Controller pController = new ProportionalController(\"P-Level\", 6, 0.5, -0.1, 0.1);\n",
    "Controller pidController = new PIDController(\"PID-Level\", 6, 0.3, 0.1, 0.05, -0.1, 0.1, 1.0);\n",
    "\n",
    "// Run and compare\n",
    "EpisodeRunner runner = new EpisodeRunner(env).setVerbose(true);\n",
    "\n",
    "List<Controller> controllers = List.of(pController, pidController, \n",
    "    new RandomController(\"Random\", -0.1, 0.1));\n",
    "\n",
    "List<BenchmarkResult> results = runner.compareControllers(controllers, 10, 500);\n",
    "EpisodeRunner.printComparison(results);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23fd63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Java controllers can also be accessed from Python via JPype\n",
    "from neqsim.process.ml.controllers import ProportionalController, PIDController, RandomController\n",
    "from neqsim.process.ml import EpisodeRunner\n",
    "\n",
    "# Create environment and controllers\n",
    "env = SeparatorGymEnv()\n",
    "env.setMaxEpisodeSteps(100)\n",
    "\n",
    "# P-Controller: index 6 is level_error, Kp=0.5, action bounds [-0.1, 0.1]\n",
    "p_controller = ProportionalController(\"P-Level\", 6, 0.5, -0.1, 0.1)\n",
    "\n",
    "# PID Controller: Kp=0.3, Ki=0.1, Kd=0.05, dt=1.0\n",
    "pid_controller = PIDController(\"PID-Level\", 6, 0.3, 0.1, 0.05, -0.1, 0.1, 1.0)\n",
    "\n",
    "# Random baseline\n",
    "random_controller = RandomController(\"Random\", -0.1, 0.1)\n",
    "\n",
    "# Create runner\n",
    "runner = EpisodeRunner(env)\n",
    "\n",
    "# Run single episode with P controller\n",
    "result = runner.runEpisode(p_controller, 100)\n",
    "print(f\"P-Controller Episode:\")\n",
    "print(f\"  Steps: {result.steps}\")\n",
    "print(f\"  Total reward: {result.totalReward:.2f}\")\n",
    "print(f\"  Mean reward: {result.getMeanReward():.3f}\")\n",
    "print(f\"  Terminated: {result.terminated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d3545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark multiple controllers\n",
    "from java.util import ArrayList\n",
    "\n",
    "controllers = ArrayList()\n",
    "controllers.add(p_controller)\n",
    "controllers.add(pid_controller)\n",
    "controllers.add(random_controller)\n",
    "\n",
    "# Run 5 episodes each, max 100 steps\n",
    "benchmark_results = runner.compareControllers(controllers, 5, 100)\n",
    "\n",
    "# Print comparison table\n",
    "EpisodeRunner.printComparison(benchmark_results)\n",
    "\n",
    "# Access individual results\n",
    "print(\"\\nDetailed Results:\")\n",
    "for result in benchmark_results:\n",
    "    print(f\"  {result.controllerName}: mean={result.meanReward:.2f}, \"\n",
    "          f\"std={result.stdReward:.2f}, success={result.successRate*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e056ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and plot trajectory from episode result\n",
    "result = runner.runEpisode(pid_controller, 100)\n",
    "\n",
    "# Get level trajectory (feature index 0)\n",
    "level_trajectory = np.array(result.getFeatureTrajectory(0))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(level_trajectory, 'b-', linewidth=2)\n",
    "axes[0].axhline(y=0.5, color='r', linestyle='--', label='Setpoint')\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Liquid Level')\n",
    "axes[0].set_title(f'PID Controller Level Control (Reward: {result.totalReward:.1f})')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot rewards over time\n",
    "rewards = [result.rewards.get(i) for i in range(int(result.steps))]\n",
    "axes[1].plot(np.cumsum(rewards), 'g-', linewidth=2)\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('Cumulative Reward')\n",
    "axes[1].set_title('Reward Accumulation')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6700e256",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Key Classes\n",
    "\n",
    "| Class | Purpose |\n",
    "|-------|---------|\n",
    "| `StateVector` | Normalized state representation with bounds |\n",
    "| `ActionVector` | Bounded action representation |\n",
    "| `ConstraintManager` | Safety/operational constraint handling |\n",
    "| `GymEnvironment` | Gymnasium-compatible base class |\n",
    "| `MultiAgentEnvironment` | Multi-agent coordination |\n",
    "| `TrainingDataCollector` | Surrogate model data generation |\n",
    "| `EpisodeRunner` | Java-based episode execution and benchmarking |\n",
    "\n",
    "### Java Controllers (No Python Required)\n",
    "\n",
    "| Controller | Formula | Use Case |\n",
    "|------------|---------|----------|\n",
    "| `ProportionalController` | $u = -K_p \\cdot e$ | Simple setpoint tracking |\n",
    "| `PIDController` | $u = -K_p e - K_i \\int e - K_d \\dot{e}$ | Industrial baseline |\n",
    "| `BangBangController` | On/off with deadband | Threshold control |\n",
    "| `RandomController` | $u \\sim \\text{Uniform}(a_{min}, a_{max})$ | Baseline comparison |\n",
    "\n",
    "### Equipment with StateVectorProvider\n",
    "\n",
    "- `Separator` - level, pressure, densities, flows\n",
    "- `Compressor` - pressures, temperatures, efficiency, surge margin\n",
    "- `HeatExchanger` - temperatures, duty, effectiveness\n",
    "\n",
    "### Testing Strategy\n",
    "\n",
    "1. **Unit test with Java controllers** - Fast, no Python deps\n",
    "2. **Benchmark baselines** - P, PID, random comparison\n",
    "3. **Train RL in Python** - Use stable-baselines3 or RLlib\n",
    "4. **Compare RL vs baselines** - Measure improvement\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Train RL agents** using stable-baselines3 or RLlib\n",
    "2. **Build surrogates** with collected training data\n",
    "3. **Extend to dynamic simulation** for time-stepping control\n",
    "4. **Add more equipment agents** (valves, pumps, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00207b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NeqSim ML Integration Tutorial Complete!\")\n",
    "print(\"\\nFor more information, see:\")\n",
    "print(\"  - docs/ml_integration.md\")\n",
    "print(\"  - neqsim.process.ml package JavaDoc\")\n",
    "print(\"  - https://github.com/equinor/neqsim\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
